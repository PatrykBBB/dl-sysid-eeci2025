{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2886f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc6066",
   "metadata": {},
   "source": [
    "# Meta Learning in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964438ca",
   "metadata": {},
   "source": [
    "## General setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410cab8",
   "metadata": {},
   "source": [
    "In bi-level meta-learning, the meta-objective to be minimized is:\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^b \\overbrace{\\mathcal{L} (\\mathrm{Alg}_\\theta(D^{\\rm tr}_i), D^{\\rm te}_i)}^{=J_i(\\theta)},$$\n",
    "\n",
    "i.e. we optimize the meta-parameters $\\theta$ of the parameterized algorithm $\\mathrm{Alg}$ such that $\\mathrm{Alg}_\\theta$, provides a model that, applied to a training dataset $D^{\\rm tr}_i$, generalizes well to the corresponding test dataset $D^{\\rm te}_i$, as measured by a loss $\\mathcal{L}$.\n",
    "\n",
    "* The loss $\\mathcal{L}$ measures the discrepancy between a model with parameters $\\phi = \\mathrm{Alg}_\\theta(D^{\\rm tr}_i)$ and the test dataset $D^{\\rm te}_i$\n",
    "* The loss is computed over a meta-batch containing $b$ dataset parirs $(D^{\\rm tr}_i, D^{\\rm te}_i)$\n",
    "* The datasets $D^{\\rm tr}_i$ and $D^{\\rm te}_i$ are generated by the same system\n",
    "* The datasets $D^{\\rm tr}_i$, $D^{\\rm tr}_j$ are generated by different, yet *related* systems. They may be though as sampled from a probability $p(D)$\n",
    "* We optimize with minibatch techniques and resample the datasets from $P(D)$ at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66c6941",
   "metadata": {},
   "source": [
    "## Static regression setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fae494",
   "metadata": {},
   "source": [
    "To make things more concrete, let us consider a static regression problem.\n",
    "\n",
    "* Each dataset $D$ contains $K$ input-output pairs $D = (\\mathbf{x}, \\mathbf{y}) = (x_1, y_1, \\dots, x_K, y_K)$, $x_j \\in \\mathbb{R}^{n_x}$, $y_j \\in \\mathbb{R}^{n_y}$.\n",
    "* We are given a model structure $\\hat {\\mathbf{y}} = f(\\phi, \\mathbf{x})$ that is suitable to describe the dependency $x \\rightarrow y$ *for all possible systems* in $p(D)$, given a system-specific choice of the parameters $\\phi$. \n",
    "* We aim at *learning an algorithm* $\\mathrm{Alg}_\\theta$ that provides such parameters $\\phi$ from a training dataset:\n",
    "$\\phi(D) = \\mathrm{Alg}_\\theta(D)$ by minimizing $J(\\theta).$\n",
    "\n",
    "\n",
    "Having introduced $f$, we can write the loss $\\mathcal{L}$ more explicitly as:\n",
    "$$\\mathcal{L}(\\phi, D) = \\ell(\\overbrace{f(\\phi, \\mathbf{u})}^{=\\hat {\\mathbf{y}}}, \\mathbf{y}),$$\n",
    "where $\\ell(\\cdot, \\cdot)$ is the measure of discrepancy, i.e. the mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182e74ad",
   "metadata": {},
   "source": [
    "## Model-Agnostic Meta Learning (MAML)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecbdc7",
   "metadata": {},
   "source": [
    "The celebrated MAML algorithm is based on:\n",
    "\n",
    "$$\n",
    "\\phi(\\theta, D_i^{\\rm tr}) = \\mathrm{Alg}_\\theta(D_i^{\\rm tr}) = \\theta - \\alpha \\nabla_1 \\mathcal{L}(\\theta, D_i^{\\rm tr}),\n",
    "$$\n",
    "\n",
    "i.e. it learns model parameters $\\phi$ as the result of a single gradient-descent step initialized at $\\theta$. Note that $\\nabla_1$ denotes here the gradient with respect to the first argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7181d",
   "metadata": {},
   "source": [
    "Let us drop the $i$ subscript and define $D^{\\rm tr} = (\\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr})$, $D^{\\rm te} = (\\mathbf{x}^{\\rm te}, \\mathbf{y}^{\\rm te})$. Overall, the MAML loss for a single step is:\n",
    "$$\n",
    "J_i(\\theta) = \\mathcal{L} \\bigg(\\theta - \\alpha \\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr}\\big), \\mathbf{x}^{\\rm te} \\mathbf{y}^{\\rm te} \\bigg)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e50385",
   "metadata": {},
   "source": [
    "We see that the loss $J_i$ is way more involved than the ones we have seen before. To evaluate it, we need to:\n",
    "\n",
    "1. Compute $\\mathcal{L}(\\theta, \\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr}) = \\ell(f(\\theta, \\mathbf{u}^{\\rm tr}), \\mathbf{y}^{\\rm tr})$\n",
    "2. Differentiate $\\mathcal{L}(\\theta, \\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr})$ w.r.t. $\\theta$ to obtain $\\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr})$\n",
    "3. Run a gradient descent step to compute $\\phi = \\theta - \\alpha \\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}^{\\rm tr}, \\mathbf{y}^{\\rm tr})$\n",
    "4. Compute  $\\mathcal{L}(\\phi, \\mathbf{x}^{\\rm te}, \\mathbf{y}^{\\rm te}) = \\ell(f(\\phi, \\mathbf{u}^{\\rm te}), \\mathbf{y}^{\\rm te})$\n",
    "\n",
    "Steps 2 and 3 require computing derivatives and updating model parameters *within the forward pass*. You will need some more advanced PyTorch to do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58263ff8",
   "metadata": {},
   "source": [
    "## MAML Implementation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd3b49d",
   "metadata": {},
   "source": [
    "### Meta dataset \n",
    "\n",
    "When working with synthetic data, it might be challenging to define a reasonable the dataset distribution. Here we just work with random data, for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7b71ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # number of *datasets* in a batch\n",
    "K = 5 # number of data points in each dataset\n",
    "nx = 1 # number of inputs\n",
    "ny = 1 # number of outputs\n",
    "\n",
    "# The meta batch\n",
    "batch_x = torch.randn(batch_size, 2*K, nx)\n",
    "batch_y = torch.randn(batch_size, 2*K, ny)\n",
    "\n",
    "# support set, aka context, training set\n",
    "batch_x_tr = batch_x[:, :K//2]\n",
    "batch_y_tr = batch_y[:, :K//2]\n",
    "# query set, aka query, test set\n",
    "batch_x_te = batch_x[:, K//2:]\n",
    "batch_y_te = batch_y[:, K//2:]\n",
    "\n",
    "# (batch_x_tr[i], batch_y_tr[i]) and (batch_x_te[i], batch_y_te[i]) are portions of the same dataset, hence generated by the same mechanism  \n",
    "# (batch_x[i], batch_y[i]) and (batch_x[j], batch_y[j]) are generated by different mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34875197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algorithm hyperparameters\n",
    "alpha = 0.1 # inner loop learning rate\n",
    "lr = 1e-3 # outer loop learning rate\n",
    "iters = 1000 # outer loop iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc1a531",
   "metadata": {},
   "source": [
    "### From modules to pure functions\n",
    "\n",
    "Implementation of Meta Learning in PyTorch is more convenient following a purely *functional approach*. We need models as pure python functions, where the output depends *explicitly* on parameters and inputs, i.e. something like:\n",
    "\n",
    "```python\n",
    "def mlp_fn(params, x)\n",
    "    ...\n",
    "    return y\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c53aff4",
   "metadata": {},
   "source": [
    "We have seen that ``torch.nn`` provides convenient tools to define neural networks as ``modules``, which are Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ab0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 40\n",
    "input_size = 1 # number of features\n",
    "output_size = 1\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, hidden_size)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        y = self.fc2(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e80a91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(hidden_size)\n",
    "x = torch.randn(32, 1)\n",
    "y_hat = mlp(x) # mlp called with input x, parameters are hidden inside the module\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc019da",
   "metadata": {},
   "source": [
    "\n",
    "If we do not want to write neural networks as functions from scratch (as we did in the first lecture), we can adapt an existing `nn.module` to behave \"functionally\".\n",
    "We first neet to extract the model parameters as a dictionary of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1652c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(mlp.named_parameters()) # dictionary {parameter_name: parameter_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ceabec",
   "metadata": {},
   "source": [
    "Then, we can call the model as a pure function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085186e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mlp_fn(params, x):\n",
    "    return torch.func.functional_call(mlp, params, x)\n",
    "\n",
    "mlp_fn(params, x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8963e2",
   "metadata": {},
   "source": [
    "### MAML Loss implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e3475",
   "metadata": {},
   "source": [
    "Let us define the loss as a function of parameters, inputs and outputs. Basically, we are defining $\\mathcal{L}(\\theta, \\mathbf{x}, \\mathbf{y})$ used in step 1 and 4 of the MAML loss computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f73f40f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2445, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_fn(params, x, y):\n",
    "    y_hat =  mlp_fn(params, x) # functional call to the mlp\n",
    "    loss = torch.mean((y_hat - y) ** 2) # MSE loss\n",
    "    return loss\n",
    "\n",
    "loss_fn(params, batch_x_tr[0], batch_y_tr[0]) # loss for the first training dataset in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cb9c9",
   "metadata": {},
   "source": [
    "Let us define the loss derivative with respect of the parameters as a function. Basically, we are defining $\\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}, \\mathbf{y})$ needed in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ad3db8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_fn = torch.func.grad(loss_fn, argnums=0) # takes a function in and returns the function derivative out\n",
    "params_grad = grad_fn(params, batch_x_tr[0],  batch_y_tr[0]) # a dict of gradients, one per parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21c1808",
   "metadata": {},
   "source": [
    "Let us define the gradiend update $\\phi = \\theta - \\alpha \\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}, \\mathbf{y})$ needed in step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3ca9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_update(p, x, y):\n",
    "    g = torch.func.grad(loss_fn)(p, x, y)\n",
    "    p = {k: p[k] - alpha * g[k] for k in p.keys()} # apply gd to each tensor in the dictionary\n",
    "    #gd_fun = lambda z, dz: z - alpha * dz\n",
    "    #p = torch.utils._pytree.tree_map(gd_fun, p, g) # apply one step of gradient descent\n",
    "    return p\n",
    "\n",
    "p_updated = inner_update(params, batch_x_tr[0], batch_y_tr[0]) # update the parameters of the first training dataset in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f3184c",
   "metadata": {},
   "source": [
    "We can finally define the MAML loss for a single dataset! This implements:\n",
    "\n",
    "$$J(\\theta, \\mathbf{x}_1, \\mathbf{y}_1, \\mathbf{x}_2, \\mathbf{y}_2) = \\mathcal{L}(\\theta - \\alpha \\nabla_1 \\mathcal{L}(\\theta, \\mathbf{x}_1, \\mathbf{y}_1), \\mathbf{x}_2, \\mathbf{y}_2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5659f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.7155, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maml_loss(p, x1, y1, x2, y2):\n",
    "    p2 = inner_update(p, x1, y1)\n",
    "    return loss_fn(p2, x2, y2)\n",
    "\n",
    "maml_loss(params, batch_x_tr[0], batch_y_tr[0], batch_x_te[0], batch_y_te[0]) # it works, and it is differentiable wrt p. We are almost there!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2231c2",
   "metadata": {},
   "source": [
    "We now need to vectorize with respect to the datasets in the meta-batch. The ``vmap`` function does the job!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91378736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2658, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_maml_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    maml_loss_batch = torch.func.vmap(maml_loss, in_dims=(None, 0, 0, 0, 0)) # vectorize wrt all but the first argument\n",
    "    batch_losses = maml_loss_batch(p, x1_b, y1_b, x2_b, y2_b)\n",
    "    return torch.mean(batch_losses)\n",
    "\n",
    "batched_maml_loss(params, batch_x_tr, batch_y_tr, batch_x_te, batch_y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13252638",
   "metadata": {},
   "source": [
    "### MAML Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52b332",
   "metadata": {},
   "source": [
    "This part is pretty standard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0a046d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(hidden_size)\n",
    "params_maml = dict(mlp.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0416159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_datasets(batch_size, K):\n",
    "    # Dummy data here. In practice, this could call a simulator from a well-tuned distribution\n",
    "    # or retrieve some real datasets of similar systems\n",
    "    batch_x = torch.randn(batch_size, 2*K, nx)\n",
    "    batch_y = torch.randn(batch_size, 2*K, ny)\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4fcded",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(params_maml.values(), lr=lr)\n",
    "losses = []\n",
    "\n",
    "for i in range(iters):\n",
    "    batch_x, batch_y = sample_datasets(batch_size=batch_size, K=2*K)\n",
    "    batch_x_tr = batch_x[:, :K]\n",
    "    batch_y_tr = batch_y[:, :K]\n",
    "    batch_x_te = batch_x[:, K:]\n",
    "    batch_y_te = batch_y[:, K:]\n",
    "\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss = batched_maml_loss(params_maml, batch_x_tr, batch_y_tr, batch_x_te, batch_y_te)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99792d12",
   "metadata": {},
   "source": [
    "## Training models starting from the MAML initialization\n",
    "\n",
    "Given a *new* dataset, we would then execute one step of gradient descent starting from the MAML initialization. In other works, run MAML's ``inner_update`` starting from the learned MALM initialization, instead of training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3a18c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = torch.randn(batch_size, K, nx)\n",
    "y_new = torch.randn(batch_size, K, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f75de919",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_params_maml = inner_update(params_maml, x_new, y_new) # update the parameters of the first training dataset in the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b52c8b",
   "metadata": {},
   "source": [
    "## Black-box meta learning with Hypernetworks\n",
    "\n",
    "MAML is just one possible meta-learning algorithm. For instance, we could parameterize \n",
    "$$\\phi(\\theta, D_i^{\\rm tr}) = \\mathrm{Alg}_\\theta(D_i^{\\rm tr})$$\n",
    "as a neural network with parameters $\\theta$ that that processes a training dataset and returns the \n",
    "parameters of *another* neural network, which models the dependency $x \\rightarrow y$. Such a network is called a Hyper-Nerwork!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18e617",
   "metadata": {},
   "source": [
    "To implement such a Hyper-Network, we need some functions to transform a flat tensor to model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d202afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = MLP(hidden_size)\n",
    "flat_params_mlp = nn.utils.parameters_to_vector(mlp.parameters()) # mlp parameters flattened to a vector\n",
    "n_params_mlp = flat_params_mlp.shape[0] \n",
    "n_params_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feeed04",
   "metadata": {},
   "source": [
    "This function does the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "593f6d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unflatten_like(flat_tensor: torch.Tensor, model: torch.nn.Module):\n",
    "    param_shapes = [p.shape for p in model.parameters()]\n",
    "    param_numels = [p.numel() for p in model.parameters()]\n",
    "    param_names = [name for name, _ in model.named_parameters()]\n",
    "    \n",
    "    split_tensors = torch.split(flat_tensor, param_numels)\n",
    "    params = {name: t.view(shape) for t, shape, name in zip(split_tensors, param_shapes, param_names)}\n",
    "    return params\n",
    "\n",
    "params_mlp_unf = unflatten_like(flat_params_mlp, mlp)  # check that the unflattening works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd0ef7",
   "metadata": {},
   "source": [
    "A Deep Set neural network that acts as a Hyper-Netwoks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af65eb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch implementation of DeepSet for generating MLP parameters\n",
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Shared MLP for set elements\n",
    "        self.fc1 = nn.Linear(2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # MLP for aggregated representation\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z = torch.cat([x, y], dim=-1)\n",
    "        z = self.fc1(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = torch.relu(z)\n",
    "        # Aggregate (sum) over set elements\n",
    "        z = torch.sum(z, dim=-2)\n",
    "        z = self.fc3(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.fc4(z) * 0.1  # scale for decent MLP initialization\n",
    "        \n",
    "        # Reshape as mlp parameters\n",
    "        z = unflatten_like(z, mlp)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5946b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the hypernetwork\n",
    "hypernet = DeepSet(hidden_size=128, output_size=n_params_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3744a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypernet usage: dataset in, MLP parameters out\n",
    "params_mlp_ = hypernet(batch_x_tr[0], batch_y_tr[0])  # Dataset in, MLP parameters out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04d4428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to make functional calls to the hypernetwork\n",
    "params_hypernet = dict(hypernet.named_parameters())\n",
    "params_mlp_ = torch.func.functional_call(hypernet, params_hypernet, args=(batch_x_tr[0], batch_y_tr[0]))  # Get the parameters for the MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad4927",
   "metadata": {},
   "source": [
    "Let us define and test the Hypernet loss for a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1c93a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2821, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hypernet_loss(ph, x1, y1, x2, y2):\n",
    "\n",
    "    # Generate the weights using the hypernetwork on the support set\n",
    "    pm = torch.func.functional_call(hypernet, ph, args=(x1, y1))\n",
    "    # Compute the loss on the query set\n",
    "    return loss_fn(pm, x2, y2)  # Loss for the second task\n",
    "\n",
    "hypernet_loss(params_hypernet, batch_x_tr[0], batch_y_tr[0], batch_x_te[0], batch_y_te[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e457042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9917, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batched_hypernet_loss(p, x1_b, y1_b, x2_b, y2_b):\n",
    "    hypernet_loss_cfg = partial(hypernet_loss, p) # fix first argument\n",
    "    hypernet_loss_batch = torch.func.vmap(hypernet_loss_cfg) # vmap over the rest\n",
    "    batch_losses = hypernet_loss_batch(x1_b, y1_b, x2_b, y2_b)\n",
    "    return torch.mean(batch_losses)\n",
    "\n",
    "batched_hypernet_loss(params_hypernet, batch_x_tr, batch_y_tr, batch_x_te, batch_y_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ad0e3b",
   "metadata": {},
   "source": [
    "The rest is equal to MAML!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
