{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2886f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc6066",
   "metadata": {},
   "source": [
    "# Meta Learning\n",
    "\n",
    "\n",
    "* In meta learning we have access to a *meta dataset*, namely a collection (possibly unlimited) of datasets\n",
    "\n",
    "$$\\mathcal{D} = \\{D_1, D_2, \\dots, \\}$$\n",
    "\n",
    "* In the case of static regression, each dataset $D_i$ is an unordered collection of $K$ input-output pairs.\n",
    "$$D_i = \\{(x_{i,1}, y_{i,1}), (x_{i,2}, y_{i,2}), \\dots, (x_{i,K}, y_{i,K})\\}, \\qquad x_{i,j} \\in \\mathbb{R}^{n_x},\\; y_{i,j} \\in \\mathbb{R}^{n_y}$$\n",
    "\n",
    "* The datasets $D_i$ are assumed to be *similar* to each other. They are thought as realizations from a probability distribution $p(D)$.\n",
    "\n",
    "Meta learning aims to improve our abilty to model the $x \\rightarrow y$ relationship while observing more datasets from $p(D)$\n",
    "\n",
    "## Dataset Spitting\n",
    "Most meta learning algorithms require to split each dataset in the meta-dataset in training (support set, context) and test (query) portions:\n",
    "\n",
    "$$\\mathcal{D} = \\{(D^{\\rm tr}_1, D^{\\rm te}_1), (D^{\\rm tr}_2, D^{\\rm te}_2), \\dots \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ebcafc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## In-context learning\n",
    "\n",
    "In-context learning is a meta-learning approach that trains a single meta-model (in-context learner) for all datasets in the meta-dataset in $\\mathcal{D}$ (equivalently, in the distribution $p(D)$.\n",
    "\n",
    "In the case of static regression, an in-context learner processes:\n",
    "\n",
    "* An input-output training dataset $D$\n",
    "* An input value $x$\n",
    "\n",
    "and produces output predictions $\\hat y$ for the input value $x$. Formally, we have:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\hat y = \\mathcal{M}(D, x)\n",
    "\\end{equation*}\n",
    "\n",
    "Training can be performed in a classic, standard supervised setting:\n",
    "\n",
    "\n",
    "$$J(\\theta) = \\sum_{i=1}^b \\sum_{j=1}^K \\ell(y^{\\rm te}_{i,j}, \\mathcal{M}(D^{\\rm tr}_i, x^{\\rm te}_{i,j})).$$\n",
    "\n",
    "The peculiar aspect of the in-context learner is that it digests whole training datasets, instead of just individial data points!\n",
    "\n",
    "## Architecture for static regression\n",
    "\n",
    "In practice, for in-context static regression, a deep set network may be used to process $D_{\\rm tr}$ and produce a fixed-size embedding $c \\in \\mathbb{R}^{n_{ctx}}$. Then, a multi-layer perceptron can be used to process $c$ and $x$ and generate the output prediction $\\hat y$. \n",
    "\n",
    "\\begin{align*}\n",
    "c &= \\mathrm{DeepSet}(D_{\\rm tr}) \\\\\n",
    "\\hat y &= \\mathrm{MLP}(c, x).\n",
    "\\end{align*}\n",
    "\n",
    "Overall, this may be seen as an encoder-decoder architecture (deep set encoder, MLP decoder). Note that the deep set architecture is a suitable encoder due to its *permutation-invariance* property.\n",
    "\n",
    "For time series data (and thus, for system identification) encoder-decoder Transformer architectures have been used in [recent works](https://ieeexplore.ieee.org/abstract/document/10324309?casa_token=QUPQER5d90cAAAAA:lg7hDHBnE-0HXc5kizO96eJmpkQFYcB6Q9fcEeneJ3aTCBXyZ7quQ07ykWgGspiWF5XoMc7qUQ)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964438ca",
   "metadata": {},
   "source": [
    "## PyTorch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8d80d2",
   "metadata": {},
   "source": [
    "General meta-learning settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd928990",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # number of *datasets* in a batch\n",
    "K = 5 # number of data points in each dataset\n",
    "n_x = 1 # number of inputs\n",
    "n_y = 1 # number of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b23c7d",
   "metadata": {},
   "source": [
    "An crucial part of meta-learning is the code that generates data from a meaningful distribution. Here we just generate random data for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7b17ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_datasets(batch_size, K):\n",
    "    # Dummy data here. In practice, this could call a simulator from a well-tuned distribution\n",
    "    # or retrieve some real datasets of similar systems\n",
    "    batch_x = torch.randn(batch_size, K, n_x)\n",
    "    batch_y = torch.randn(batch_size, K, n_y)\n",
    "    return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b32b440",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = sample_datasets(batch_size=batch_size, K=2*K)\n",
    "# support set, aka context, training set\n",
    "batch_x_tr = batch_x[:, :K]\n",
    "batch_y_tr = batch_y[:, :K]\n",
    "# query set, aka query, test set\n",
    "batch_x_te = batch_x[:, K:]\n",
    "batch_y_te = batch_y[:, K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375eee43-7f7e-42a0-b13f-859b1bed3234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5681b25-76e3-4a19-813e-13885c5fe3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ctx = 20 # size of the context embedding ctx = encoder(x_tr, y_tr)\n",
    "hidden_size = 40 # size of all hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14b2df",
   "metadata": {},
   "source": [
    "Let us define the encoder architecture as a deep set network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "464f6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, n_x, n_y, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Shared MLP for set elements\n",
    "        self.fc1 = nn.Linear(n_x + n_y, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        # MLP for aggregated representation\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc4 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        z = torch.cat([x, y], dim=-1)\n",
    "        z = self.fc1(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.fc2(z)\n",
    "        z = torch.relu(z)\n",
    "        # Aggregate (sum) over set elements\n",
    "        z = torch.sum(z, dim=-2)\n",
    "        z = self.fc3(z)\n",
    "        z = torch.relu(z)\n",
    "        z = self.fc4(z)\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd37752c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = DeepSet(n_x, n_y, hidden_size, output_size=n_ctx)\n",
    "ctx = encoder(batch_x_tr, batch_y_tr)\n",
    "ctx.shape # describe each dataset as a vector of size n_ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9030e5-1da2-4163-af5f-19a2ae931184",
   "metadata": {},
   "source": [
    "Permutation-invariance illustration. The order of the $K$ observations in each dataset do not influence the encoder output thanks to its deep set architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de05c66f-5607-43b6-809a-c92ba9ffe5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = torch.randperm(K)\n",
    "ctx_p = encoder(batch_x_tr[:, p, :], batch_y_tr[:, p, :])\n",
    "torch.allclose(ctx, ctx_p, atol=1e-6) # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece65715",
   "metadata": {},
   "source": [
    "Let us define the decoder architecture as a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c79cf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_x, n_ctx, n_y, hidden_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(n_x + n_ctx, hidden_size)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.fc3 = nn.Linear(hidden_size, n_y)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        \"\"\"\n",
    "        x: input data (batch_size, K, n_x)\n",
    "        c: context embedding (batch_size, n_ctx)\n",
    "        \"\"\"\n",
    "        \n",
    "        c_rep = c.unsqueeze(-2).repeat(1, x.shape[1], 1)\n",
    "        xc = torch.cat((x, c_rep), dim=-1)\n",
    "        z = self.fc1(xc)\n",
    "        z = self.act1(z)\n",
    "        z = self.fc2(z)\n",
    "        z = self.act2(z)\n",
    "        z = self.fc3(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95a301a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = MLP(n_x, n_ctx, n_y, hidden_size)\n",
    "y_hat = decoder(batch_x_te, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4827721",
   "metadata": {},
   "source": [
    "Let us jointly train encoder and decoder networks on the dataset distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afec69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee19d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr)\n",
    "losses = []\n",
    "\n",
    "\n",
    "for itr in range(iters):\n",
    "    \n",
    "    batch_x, batch_y = sample_datasets(batch_size=batch_size, K=2*K)\n",
    "    batch_x_tr = batch_x[:, :K]\n",
    "    batch_y_tr = batch_y[:, :K]\n",
    "    batch_x_te = batch_x[:, K:]\n",
    "    batch_y_te = batch_y[:, K:]\n",
    "\n",
    "    opt.zero_grad()\n",
    "    ctx = encoder(batch_x_tr, batch_y_tr)\n",
    "    batch_y_te_hat = decoder(batch_x_te, ctx)\n",
    "    loss = torch.mean((batch_y_te_hat - batch_y_te) ** 2)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    losses.append(loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
